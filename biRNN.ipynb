{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-directional RNN example using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "a bidirectional RNN/LSTM implementation using tensorflow\n",
    "on MNIST dataset\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-846ffadd6593>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/harianja/deep_learning/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/harianja/deep_learning/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist_dataset/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/harianja/deep_learning/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist_dataset/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/harianja/deep_learning/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist_dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_dataset/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/harianja/deep_learning/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- import dataset\n",
    "'''\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_dataset = input_data.read_data_sets('./mnist_dataset/', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to classify images using bidirectional LSTM, we consider every image row as sequence\n",
    "of pixels. Because MNIST image shape is 28*28px, we will then handle \n",
    "28 sequences for 28 steps for every sample (to match 28x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "training parameters\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "network parameters\n",
    "'''\n",
    "num_input = 28 # MNIST data input (img: 28)\n",
    "timesteps = 28\n",
    "num_hidden = 128\n",
    "num_classes = 10 #0-9 of one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "placeholders for inputs and targets, weights and biases\n",
    "'''\n",
    "inputs = tf.placeholder(\"float32\", [None, timesteps, num_input])\n",
    "targets = tf.placeholder(\"float32\", [None, num_classes])\n",
    "'''\n",
    "weights => 2*num_hidden because we have forward and backward\n",
    "'''\n",
    "weights = tf.Variable(tf.random_normal([2 * num_hidden, num_classes]))\n",
    "bias = tf.Variable(tf.random_normal([num_classes]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BiRNN_model(inputs, weights, bias):\n",
    "    '''\n",
    "    - prepare data shape to match RNN function requirements\n",
    "    - current data input shape: (batch_size, timesteps = 0, n_input)\n",
    "    - required shape: 'timesteps' tensor list of shape (batch_size, num_input)\n",
    "    - 28 of (128, 28)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    - unstack to get a list of 'timesteps' tensor of shape (batch_size, num_input)\n",
    "    '''\n",
    "    lstm_inputs = tf.unstack(inputs, timesteps, 1)\n",
    "    \n",
    "    '''\n",
    "    - define both forward and backward lstm cell\n",
    "    '''\n",
    "    lstm_forward_cell = rnn.BasicLSTMCell(num_hidden, forget_bias = 1.0)\n",
    "    lstm_backward_cell = rnn.BasicLSTMCell(num_hidden, forget_bias = 1.0)\n",
    "    \n",
    "    '''\n",
    "    get output, no need final state\n",
    "    '''\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_forward_cell,\n",
    "                                                           lstm_backward_cell, \n",
    "                                                           lstm_inputs, dtype = tf.float32)\n",
    "    #return as logits for softmax function\n",
    "    #only use last output\n",
    "    return tf.matmul(outputs[-1], weights) + bias \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = BiRNN_model(inputs, weights, bias)\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "loss and optimizer, using logits above\n",
    "'''\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                         logits = logits, labels = targets))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "evaluate model\n",
    "'''\n",
    "correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/10000 loss: 2.975, accuracy: 0.047\n",
      "epoch: 100/10000 loss: 2.285, accuracy: 0.172\n",
      "epoch: 200/10000 loss: 2.142, accuracy: 0.211\n",
      "epoch: 300/10000 loss: 1.959, accuracy: 0.406\n",
      "epoch: 400/10000 loss: 2.068, accuracy: 0.367\n",
      "epoch: 500/10000 loss: 1.897, accuracy: 0.414\n",
      "epoch: 600/10000 loss: 1.775, accuracy: 0.438\n",
      "epoch: 700/10000 loss: 1.681, accuracy: 0.562\n",
      "epoch: 800/10000 loss: 1.615, accuracy: 0.562\n",
      "epoch: 900/10000 loss: 1.725, accuracy: 0.461\n",
      "epoch: 1000/10000 loss: 1.520, accuracy: 0.516\n",
      "epoch: 1100/10000 loss: 1.578, accuracy: 0.539\n",
      "epoch: 1200/10000 loss: 1.553, accuracy: 0.453\n",
      "epoch: 1300/10000 loss: 1.318, accuracy: 0.641\n",
      "epoch: 1400/10000 loss: 1.473, accuracy: 0.562\n",
      "epoch: 1500/10000 loss: 1.377, accuracy: 0.609\n",
      "epoch: 1600/10000 loss: 1.392, accuracy: 0.586\n",
      "epoch: 1700/10000 loss: 1.550, accuracy: 0.508\n",
      "epoch: 1800/10000 loss: 1.247, accuracy: 0.578\n",
      "epoch: 1900/10000 loss: 1.263, accuracy: 0.594\n",
      "epoch: 2000/10000 loss: 1.331, accuracy: 0.555\n",
      "epoch: 2100/10000 loss: 1.181, accuracy: 0.625\n",
      "epoch: 2200/10000 loss: 1.245, accuracy: 0.648\n",
      "epoch: 2300/10000 loss: 1.164, accuracy: 0.656\n",
      "epoch: 2400/10000 loss: 1.186, accuracy: 0.617\n",
      "epoch: 2500/10000 loss: 0.994, accuracy: 0.688\n",
      "epoch: 2600/10000 loss: 1.123, accuracy: 0.633\n",
      "epoch: 2700/10000 loss: 1.196, accuracy: 0.594\n",
      "epoch: 2800/10000 loss: 1.096, accuracy: 0.633\n",
      "epoch: 2900/10000 loss: 1.040, accuracy: 0.695\n",
      "epoch: 3000/10000 loss: 1.119, accuracy: 0.664\n",
      "epoch: 3100/10000 loss: 1.165, accuracy: 0.633\n",
      "epoch: 3200/10000 loss: 1.029, accuracy: 0.703\n",
      "epoch: 3300/10000 loss: 0.933, accuracy: 0.680\n",
      "epoch: 3400/10000 loss: 0.878, accuracy: 0.742\n",
      "epoch: 3500/10000 loss: 0.862, accuracy: 0.742\n",
      "epoch: 3600/10000 loss: 0.847, accuracy: 0.695\n",
      "epoch: 3700/10000 loss: 1.084, accuracy: 0.734\n",
      "epoch: 3800/10000 loss: 1.015, accuracy: 0.648\n",
      "epoch: 3900/10000 loss: 0.850, accuracy: 0.711\n",
      "epoch: 4000/10000 loss: 0.921, accuracy: 0.703\n",
      "epoch: 4100/10000 loss: 0.966, accuracy: 0.672\n",
      "epoch: 4200/10000 loss: 0.855, accuracy: 0.734\n",
      "epoch: 4300/10000 loss: 0.795, accuracy: 0.711\n",
      "epoch: 4400/10000 loss: 0.792, accuracy: 0.742\n",
      "epoch: 4500/10000 loss: 0.777, accuracy: 0.758\n",
      "epoch: 4600/10000 loss: 0.718, accuracy: 0.781\n",
      "epoch: 4700/10000 loss: 0.832, accuracy: 0.727\n",
      "epoch: 4800/10000 loss: 0.839, accuracy: 0.750\n",
      "epoch: 4900/10000 loss: 0.836, accuracy: 0.711\n",
      "epoch: 5000/10000 loss: 0.842, accuracy: 0.703\n",
      "epoch: 5100/10000 loss: 0.632, accuracy: 0.805\n",
      "epoch: 5200/10000 loss: 0.702, accuracy: 0.758\n",
      "epoch: 5300/10000 loss: 0.876, accuracy: 0.711\n",
      "epoch: 5400/10000 loss: 0.681, accuracy: 0.812\n",
      "epoch: 5500/10000 loss: 0.758, accuracy: 0.742\n",
      "epoch: 5600/10000 loss: 0.584, accuracy: 0.836\n",
      "epoch: 5700/10000 loss: 0.744, accuracy: 0.773\n",
      "epoch: 5800/10000 loss: 0.474, accuracy: 0.867\n",
      "epoch: 5900/10000 loss: 0.724, accuracy: 0.836\n",
      "epoch: 6000/10000 loss: 0.580, accuracy: 0.812\n",
      "epoch: 6100/10000 loss: 0.597, accuracy: 0.852\n",
      "epoch: 6200/10000 loss: 0.760, accuracy: 0.781\n",
      "epoch: 6300/10000 loss: 0.621, accuracy: 0.781\n",
      "epoch: 6400/10000 loss: 0.662, accuracy: 0.805\n",
      "epoch: 6500/10000 loss: 0.554, accuracy: 0.852\n",
      "epoch: 6600/10000 loss: 0.651, accuracy: 0.766\n",
      "epoch: 6700/10000 loss: 0.661, accuracy: 0.797\n",
      "epoch: 6800/10000 loss: 0.538, accuracy: 0.828\n",
      "epoch: 6900/10000 loss: 0.502, accuracy: 0.836\n",
      "epoch: 7000/10000 loss: 0.531, accuracy: 0.828\n",
      "epoch: 7100/10000 loss: 0.674, accuracy: 0.844\n",
      "epoch: 7200/10000 loss: 0.605, accuracy: 0.797\n",
      "epoch: 7300/10000 loss: 0.735, accuracy: 0.734\n",
      "epoch: 7400/10000 loss: 0.706, accuracy: 0.805\n",
      "epoch: 7500/10000 loss: 0.667, accuracy: 0.773\n",
      "epoch: 7600/10000 loss: 0.525, accuracy: 0.828\n",
      "epoch: 7700/10000 loss: 0.577, accuracy: 0.789\n",
      "epoch: 7800/10000 loss: 0.489, accuracy: 0.836\n",
      "epoch: 7900/10000 loss: 0.577, accuracy: 0.797\n",
      "epoch: 8000/10000 loss: 0.561, accuracy: 0.812\n",
      "epoch: 8100/10000 loss: 0.597, accuracy: 0.820\n",
      "epoch: 8200/10000 loss: 0.641, accuracy: 0.836\n",
      "epoch: 8300/10000 loss: 0.712, accuracy: 0.773\n",
      "epoch: 8400/10000 loss: 0.592, accuracy: 0.812\n",
      "epoch: 8500/10000 loss: 0.563, accuracy: 0.781\n",
      "epoch: 8600/10000 loss: 0.490, accuracy: 0.844\n",
      "epoch: 8700/10000 loss: 0.572, accuracy: 0.875\n",
      "epoch: 8800/10000 loss: 0.542, accuracy: 0.820\n",
      "epoch: 8900/10000 loss: 0.506, accuracy: 0.883\n",
      "epoch: 9000/10000 loss: 0.584, accuracy: 0.766\n",
      "epoch: 9100/10000 loss: 0.568, accuracy: 0.781\n",
      "epoch: 9200/10000 loss: 0.473, accuracy: 0.859\n",
      "epoch: 9300/10000 loss: 0.425, accuracy: 0.883\n",
      "epoch: 9400/10000 loss: 0.421, accuracy: 0.891\n",
      "epoch: 9500/10000 loss: 0.499, accuracy: 0.875\n",
      "epoch: 9600/10000 loss: 0.473, accuracy: 0.859\n",
      "epoch: 9700/10000 loss: 0.546, accuracy: 0.844\n",
      "epoch: 9800/10000 loss: 0.412, accuracy: 0.875\n",
      "epoch: 9900/10000 loss: 0.467, accuracy: 0.844\n",
      "test accuracy: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-training\n",
    "'''\n",
    "epochs = 10000\n",
    "with tf.Session() as sess:\n",
    "    #variable initializer\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_x, batch_y = mnist_dataset.train.next_batch(batch_size)\n",
    "        '''\n",
    "        - reshape batch_x to get seq of 28 x 28\n",
    "        '''\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        #loss, final_state\n",
    "        feed = {inputs: batch_x,\n",
    "                targets: batch_y}\n",
    "        loss, model_accuracy, _ = sess.run([cost, accuracy, optimizer], \n",
    "                                                  feed_dict = feed)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('epoch: {}/{} loss: {:.3f}, accuracy: {:.3f}'.\n",
    "                  format(epoch, epochs, loss, model_accuracy))\n",
    "    \n",
    "    test_len = 10\n",
    "    test_x = mnist_dataset.test.images[ : test_len].reshape((-1, timesteps, num_input))\n",
    "    test_y = mnist_dataset.test.labels[ : test_len]\n",
    "    feed = {inputs: test_x,\n",
    "            targets: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict = feed)\n",
    "    print('test accuracy: {}'.format(test_acc))\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
